# 96
telegram:
#  net: QiGi3
  layer: 4
  lr: 0.01
  BN_model: True    # sure
  First_self_loop: True
  jk: 'max'
  dropout: 0.0


# 96
WikipediaNetwork/chameleon:
  dropout: 0.0  # 0.5(acc65), 0.0(acc75)
  net:
  layer:
  lr:
  BN_model: 1 # without layerNorm, very bad
  self_loop: "remove"
  jk: 'max'





#
cora_ml:
#  net: QiGi3
  layer: 2   # 2(80), 3(75), 4(79.8), 5(79.6)
  lr: 0.01    # 0.1(79), 0.01(80.7)
  BN_model: True   # 0(29), 1(79)
  First_self_loop : 'add'  # sure
  jk: "max"     # max(80.7), cat(79.8), 0(76.9)
#  dropout: 0.5   # 0.0(80.51), 0.5(80.76)

#
citeseer_npz:
  layer:
  lr: 0.01
  BN_model:
  First_self_loop :
  jk:
#  dropout:


#
dgl/pubmed:
  net:
  layer: 4
  lr:
  BN_model: 1  # 0(33), 1(73)
  First_self_loop : True
  jk: 'max'
  dropout: 0.0  #0.5(39), 0.0(77.5)

#
WikiCS/:
  net:
  layer:
  lr:
  BN_model: 1  # 0(23), 1(73.5)
  self_loop : 'remove'  # remove(73.9), add(73.7), 0(73.85)
  jk: 'max'   # max(75.9),cat(75.99), 0(73.5)
  dropout: 0.5  # 0.5(75.9), 0.0(73.9)